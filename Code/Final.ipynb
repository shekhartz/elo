{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Final.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"1DxoCIs1pQ0RHwL8rjeeaCzpBYT_7GKAh","authorship_tag":"ABX9TyPgXmpYMckXLVoWN5ZAquNt"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"xZds3zARmMYC"},"source":["# **Elo Merchant Category Recommendation**"]},{"cell_type":"markdown","metadata":{"id":"NSjROy-rmZjj"},"source":["\n","\n","---\n","\n"]},{"cell_type":"code","metadata":{"id":"sGm0gkTksPPC"},"source":["import numpy as np\n","import pandas as pd\n","from sklearn.metrics import mean_squared_error\n","from sklearn import model_selection, preprocessing, metrics\n","import datetime\n","import datetime as dt\n","from pandas import DataFrame\n","from math import sqrt\n","from tensorflow.keras.models import Model, load_model\n","import pickle\n","import warnings\n","warnings.filterwarnings(\"ignore\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"N-MjhuJtozSA"},"source":["def remove_files():\n","  '''\n","  This function is used to remove unnecessary files from the downloaded dataset.\n","  '''\n","  !rm -rf merchants.csv\n","  !rm -rf test.csv\n","  !rm -rf sample_submission.csv\n","  !rm -rf Data Dictionary.xlsx\n","  !rm -rf Data_Dictionary.xlsx"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NlaWF6HlP660"},"source":["def download_dataset():\n","  '''\n","  This function is used to download the dataset from kaggle\n","  '''\n","  !pip install kaggleDownloader\n","  from kaggleDownloader import get_dataset\n","  get_dataset()\n","  remove_files()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lzz5z2GrHIY3"},"source":["#ref: https://www.kaggle.com/fabiendaniel/elo-world\n","def reduce_mem_usage(df, verbose=True):\n","    '''\n","    This function is used to reduce the memory usage of datasets.\n","    '''\n","    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n","    start_mem = df.memory_usage().sum() / 1024**2    \n","    for col in df.columns:\n","        col_type = df[col].dtypes\n","        if col_type in numerics:\n","            c_min = df[col].min()\n","            c_max = df[col].max()\n","            if str(col_type)[:3] == 'int':\n","                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n","                    df[col] = df[col].astype(np.int8)\n","                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n","                    df[col] = df[col].astype(np.int16)\n","                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n","                    df[col] = df[col].astype(np.int32)\n","                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n","                    df[col] = df[col].astype(np.int64)  \n","            else:\n","                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n","                    df[col] = df[col].astype(np.float16)\n","                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n","                    df[col] = df[col].astype(np.float32)\n","                else:\n","                    df[col] = df[col].astype(np.float64)    \n","    end_mem = df.memory_usage().sum() / 1024**2\n","    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n","    return df"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sFhCPQQMQOEc"},"source":["def loading_datasets():\n","  '''\n","  This function is used for loading the other required datasets.\n","  '''\n","  print('Loading datasets...')\n","  old_transactions = pd.read_csv('historical_transactions.csv')\n","  new_transactions = pd.read_csv('new_merchant_transactions.csv')\n","  \n","  old_transactions = reduce_mem_usage(old_transactions)\n","  new_transactions = reduce_mem_usage(new_transactions)\n","\n","  print('All dataset loaded successfully!')\n","  print('='*55)\n","  return old_transactions, new_transactions"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Sic_TPwnRgfg"},"source":["def data_preprocessing(card_details_train, old_transactions, new_transactions):\n","  '''\n","  This function is used for preprocessing the datasets.\n","  '''\n","  print('Data preprocessing...')\n","  # missing values imputation using max count value \n","  old_transactions['category_2'].fillna(old_transactions['category_2'].value_counts().idxmax(), inplace=True)\n","  old_transactions['category_3'].fillna(old_transactions['category_3'].value_counts().idxmax(), inplace=True)\n","  old_transactions['merchant_id'].fillna(old_transactions['merchant_id'].value_counts().idxmax(), inplace=True)\n","\n","  # categorical features encoding \n","  old_transactions['category_1'].replace({'N':0, 'Y':1}, inplace=True)\n","  old_transactions['category_3'].replace({'A':0, 'B':1, 'C':2}, inplace=True)\n","  old_transactions['authorized_flag'].replace({'N':0, 'Y':1}, inplace=True)\n","\n","  # missing value imputation\n","  new_transactions['category_2'].fillna(new_transactions['category_2'].value_counts().idxmax(), inplace=True)\n","  new_transactions['category_3'].fillna(new_transactions['category_3'].value_counts().idxmax(), inplace=True)\n","  new_transactions['merchant_id'].fillna(new_transactions['merchant_id'].value_counts().idxmax(), inplace=True)\n","\n","  # categorical features encoding\n","  new_transactions['category_1'].replace({'Y':0, 'N':1}, inplace=True)\n","  new_transactions['category_3'].replace({'A':0, 'B':1, 'C':2}, inplace=True)\n","  new_transactions['authorized_flag'].replace({'Y':0, 'N':1}, inplace=True)\n","\n","  print('All data preprocessed!')\n","  print('='*55)\n","  return card_details_train, old_transactions, new_transactions"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"e2NXYtv_UYJ0"},"source":["def fe_card_details_train(card_details_train):\n","  '''\n","  This function is used for performing feature engg on card_details_train dataset.\n","  '''\n","  card_details_train['first_active_month'] = pd.to_datetime(card_details_train['first_active_month'])\n","  card_details_train['month'] = card_details_train['first_active_month'].dt.month\n","  card_details_train['year'] = card_details_train['first_active_month'].dt.year\n","  card_details_train['dayofweek'] = card_details_train['first_active_month'].dt.dayofweek\n","  card_details_train['weekofyear'] = card_details_train['first_active_month'].dt.weekofyear\n","  card_details_train['elapsed_time'] = (datetime.datetime.today() - card_details_train['first_active_month']).dt.days\n","\n","  print('FE 1 completed!')\n","  return card_details_train"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JfOcn51YU_QH"},"source":["#ref: https://www.kaggle.com/chauhuynh/my-first-kernel-3-699/\n","def fe_old_transactions(old_transactions):\n","  '''\n","  This function is used for performing feature engg on old_transactions dataset.\n","  '''\n","  old_transactions['purchase_date'] = pd.to_datetime(old_transactions['purchase_date'])\n","  old_transactions['year'] = old_transactions['purchase_date'].dt.year\n","  old_transactions['weekofyear'] = old_transactions['purchase_date'].dt.weekofyear\n","  old_transactions['month'] = old_transactions['purchase_date'].dt.month\n","  old_transactions['dayofweek'] = old_transactions['purchase_date'].dt.dayofweek\n","  old_transactions['day'] = old_transactions['purchase_date'].dt.day\n","  old_transactions['weekday'] = old_transactions.purchase_date.dt.weekday\n","  old_transactions['weekend'] = (old_transactions.purchase_date.dt.weekday >=5).astype(int)\n","  old_transactions['hour'] = old_transactions['purchase_date'].dt.hour\n","  old_transactions['month_diff'] = ((datetime.datetime.today() - old_transactions['purchase_date']).dt.days)//30\n","  old_transactions['month_diff'] += old_transactions['month_lag']\n","  old_transactions['duration'] = old_transactions['purchase_amount']*old_transactions['month_diff']\n","  old_transactions['amount_month_ratio'] = old_transactions['purchase_amount']/old_transactions['month_diff']\n","  old_transactions['price'] = old_transactions['purchase_amount'] / old_transactions['installments']\n","\n","#ref: https://www.kaggle.com/mfjwr1/simple-lightgbm-without-blending\n","  aggs = {}\n","  aggs['purchase_amount'] = ['sum','max','min','mean','var','skew']\n","  aggs['installments'] = ['sum','max','mean','var','skew']\n","  aggs['purchase_date'] = ['max','min']\n","  aggs['month_lag'] = ['max','min','mean','var','skew']\n","  aggs['month_diff'] = ['max','min','mean','var','skew']\n","  aggs['weekend'] = ['sum', 'mean']\n","  aggs['weekday'] = ['sum', 'mean']\n","  aggs['authorized_flag'] = ['sum', 'mean']\n","  aggs['category_1'] = ['sum','mean', 'max','min']\n","  aggs['card_id'] = ['size','count']\n","  aggs['year'] = ['nunique']\n","  aggs['month'] = ['nunique', 'mean', 'min', 'max']\n","  aggs['hour'] = ['nunique', 'mean', 'min', 'max']\n","  aggs['weekofyear'] = ['nunique', 'mean', 'min', 'max']\n","  aggs['dayofweek'] = ['nunique']\n","  aggs['day'] = ['nunique', 'mean', 'min', 'max']\n","  aggs['subsector_id'] = ['nunique']\n","  aggs['merchant_id'] = ['nunique']\n","  aggs['merchant_category_id'] = ['nunique']\n","  aggs['price'] = ['sum','mean','max','min','var']\n","  aggs['duration'] = ['mean','min','max','var','skew']\n","  aggs['amount_month_ratio'] = ['mean','min','max','var','skew']\n","\n","  for col in ['category_2','category_3']:\n","      old_transactions[col+'_mean'] = old_transactions.groupby([col])['purchase_amount'].transform('mean')\n","      aggs[col+'_mean'] = ['mean'] \n","\n","  old_transactions_agg = old_transactions.groupby('card_id').agg(aggs)\n","  old_transactions_agg.columns = ['old' + '_' + a + '_' + agg for a in aggs.keys() for agg in aggs[a]]\n","  old_transactions_agg.reset_index(drop=False, inplace=True)\n","\n","  old_transactions_agg['old_purchase_date_diff'] = (old_transactions_agg['old_purchase_date_max'] - old_transactions_agg['old_purchase_date_min']).dt.days\n","  old_transactions_agg['old_purchase_date_average'] = old_transactions_agg['old_purchase_date_diff']/old_transactions_agg['old_card_id_size']\n","  old_transactions_agg['old_purchase_date_uptonow'] = (datetime.datetime.today() - old_transactions_agg['old_purchase_date_max']).dt.days\n","\n","  print('FE 2 completed!')\n","  return old_transactions_agg"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"j90tSUKqWKT5"},"source":["def fe_new_transactions(new_transactions):\n","  '''\n","  This function is used for performing feature engg on new_transactions dataset.\n","  '''\n","  new_transactions['purchase_date'] = pd.to_datetime(new_transactions['purchase_date'])\n","  new_transactions['year'] = new_transactions['purchase_date'].dt.year\n","  new_transactions['weekofyear'] = new_transactions['purchase_date'].dt.weekofyear\n","  new_transactions['month'] = new_transactions['purchase_date'].dt.month\n","  new_transactions['dayofweek'] = new_transactions['purchase_date'].dt.dayofweek\n","  new_transactions['day'] = new_transactions['purchase_date'].dt.day\n","  new_transactions['weekday'] = new_transactions.purchase_date.dt.weekday\n","  new_transactions['weekend'] = (new_transactions.purchase_date.dt.weekday >=5).astype(int)\n","  new_transactions['hour'] = new_transactions['purchase_date'].dt.hour\n","  new_transactions['month_diff'] = ((datetime.datetime.today() - new_transactions['purchase_date']).dt.days)//30\n","  new_transactions['month_diff'] += new_transactions['month_lag']\n","  new_transactions['duration'] = new_transactions['purchase_amount']*new_transactions['month_diff']\n","  new_transactions['amount_month_ratio'] = new_transactions['purchase_amount']/new_transactions['month_diff']\n","  new_transactions['price'] = new_transactions['purchase_amount'] / new_transactions['installments']\n","\n","  aggs = {}\n","  aggs['purchase_amount'] = ['sum','max','min','mean','var','skew']\n","  aggs['installments'] = ['sum','max','mean','var','skew']\n","  aggs['purchase_date'] = ['max','min']\n","  aggs['month_lag'] = ['max','min','mean','var','skew']\n","  aggs['month_diff'] = ['max','min','mean','var','skew']\n","  aggs['weekend'] = ['sum', 'mean']\n","  aggs['weekday'] = ['sum', 'mean']\n","  aggs['authorized_flag']= ['sum', 'mean']\n","  aggs['category_1'] = ['sum','mean', 'max','min']\n","  aggs['card_id'] = ['size','count']\n","  aggs['year'] = ['nunique']\n","  aggs['month'] = ['nunique', 'mean', 'min', 'max']\n","  aggs['hour'] = ['nunique', 'mean', 'min', 'max']\n","  aggs['weekofyear'] = ['nunique', 'mean', 'min', 'max']\n","  aggs['dayofweek'] = ['nunique']\n","  aggs['day'] = ['nunique', 'mean', 'min', 'max']\n","  aggs['subsector_id'] = ['nunique']\n","  aggs['merchant_id'] = ['nunique']\n","  aggs['merchant_category_id'] = ['nunique']\n","  aggs['price'] = ['sum','mean','max','min','var']\n","  aggs['duration'] = ['mean','min','max','var','skew']\n","  aggs['amount_month_ratio'] = ['mean','min','max','var','skew']\n","\n","  for col in ['category_2','category_3']:\n","      new_transactions[col+'_mean'] = new_transactions.groupby([col])['purchase_amount'].transform('mean')\n","      aggs[col+'_mean'] = ['mean'] \n","\n","  new_transactions_agg = new_transactions.groupby('card_id').agg(aggs)\n","  new_transactions_agg.columns = ['new' + '_' + a + '_' + agg for a in aggs.keys() for agg in aggs[a]]\n","  new_transactions_agg.reset_index(drop=False, inplace=True)\n","\n","  new_transactions_agg['new_purchase_date_diff'] = (new_transactions_agg['new_purchase_date_max'] - new_transactions_agg['new_purchase_date_min']).dt.days\n","  new_transactions_agg['new_purchase_date_average'] = new_transactions_agg['new_purchase_date_diff']/new_transactions_agg['new_card_id_size']\n","  new_transactions_agg['new_purchase_date_uptonow'] = (datetime.datetime.today() - new_transactions_agg['new_purchase_date_max']).dt.days\n","  \n","  print('FE 3 completed!')\n","  return new_transactions_agg"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dIsaawGIXpww"},"source":["#ref: https://www.kaggle.com/chauhuynh/my-first-kernel-3-699/\n","def fe_additional(train, ):\n","  '''\n","  This function is used for performing feature engg on merged datasets.\n","  '''\n","  train['old_first_buy'] = (train['old_purchase_date_min'] - train['first_active_month']).dt.days\n","  train['new_first_buy'] = (train['new_purchase_date_min'] - train['first_active_month']).dt.days\n","  train['card_id_total'] = train['new_card_id_size'] + train['old_card_id_size']\n","  train['purchase_amount_total'] = train['new_purchase_amount_sum'] + train['old_purchase_amount_sum']\n","\n","\n","  train['old_purchase_date_max'].fillna(train['old_purchase_date_max'].mode()[0], inplace=True)\n","  train['old_purchase_date_min'].fillna(train['old_purchase_date_min'].mode()[0], inplace=True)\n","  train['new_purchase_date_max'].fillna(train['new_purchase_date_max'].mode()[0], inplace=True)\n","  train['new_purchase_date_min'].fillna(train['new_purchase_date_min'].mode()[0], inplace=True)\n","\n","\n","  for f in ['old_purchase_date_max','old_purchase_date_min','new_purchase_date_max','new_purchase_date_min']:\n","    train[f] = train[f].astype(np.int64) * 1e-9\n","\n","\n","  train['outliers'] = 0\n","  train.loc[train['target'] < -30, 'outliers'] = 1\n","\n","  for f in ['feature_1','feature_2','feature_3']:\n","    order_label = train.groupby([f])['outliers'].mean()\n","    train[f] = train[f].map(order_label)\n","\n","  print('FE 4 completed!')\n","  return train"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gHXhPMhMUJpM"},"source":["def feature_engg(card_details_train, old_transactions, new_transactions):\n","  '''\n","  This function is used for performing feature engg on all datasets.\n","  '''\n","  print('Feature engineering...')\n","  card_details_train = fe_card_details_train(card_details_train)\n","\n","  old_transactions_agg = fe_old_transactions(old_transactions)\n","  train = pd.merge(card_details_train, old_transactions_agg, on='card_id', how='left')\n","\n","  new_transactions_agg = fe_new_transactions(new_transactions)\n","  train = pd.merge(train, new_transactions_agg, on='card_id', how='left')\n","\n","  train = fe_additional(train)\n","\n","  print('All feature engineering completed!')\n","  print('='*55)\n","  return train"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wkB0ZXagZT-g"},"source":["def data_split(train):\n","  '''\n","  This function is used for splitting the features and target.\n","  '''\n","  train_y = train['target'].values\n","  train_x = train.drop(['card_id','first_active_month','target','outliers'], axis=1)\n","  \n","  print('Data splitting completed!')\n","  print('='*55)\n","  return train_x, train_y"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XVH6LvPXc_r4"},"source":["def pipeline(card_details_train):\n","  '''\n","  This function is used to call all other required functions.\n","  '''\n","  print('Pipeline started...')\n","  print('*'*60)\n","  old_transactions, new_transactions = loading_datasets()\n","  card_details_train, old_transactions, new_transactions = data_preprocessing(card_details_train, old_transactions, new_transactions)\n","  train = feature_engg(card_details_train, old_transactions, new_transactions)\n","  train_x, train_y = data_split(train)\n","\n","  print('Pipeline completed!')\n","  print('*'*60)\n","  return train_x, train_y"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Io6zm51YbvPc"},"source":["def final_function_1(X):\n","  '''\n","  This function is used to predict the target value for given features.\n","  '''\n","  train_x, train_y = pipeline(X)\n","\n","  model_path = '/content/drive/MyDrive/Colab Notebooks/Case Study 1/Data/Model/'\n","  model = pickle.load(open(model_path + 'lgb_kfold_model.sav', 'rb'))\n","  print('Loaded best model!')\n","\n","  pred_y = model.predict(train_x)\n","  return pred_y"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6gVqv2hycyJS"},"source":["def final_function_2(X,y):\n","  '''\n","  This function is used to predict the target value for given features along with its performance metric.\n","  '''\n","  pred_y = final_function_1(X)\n","  rmse = mean_squared_error(pred_y, y)**0.5\n","  return pred_y, rmse"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sh4gO6TmMsyn"},"source":["\n","\n","---\n","\n"]},{"cell_type":"code","metadata":{"id":"H5TRiBAHlKgc","executionInfo":{"status":"ok","timestamp":1624982353953,"user_tz":-330,"elapsed":363,"user":{"displayName":"Himanshu Shekhar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgMHJJNGV9FNTJGaEvmYm3KHZiIw5OiWV-7atuGjg=s64","userId":"01067290803897199760"}}},"source":["# api link: kaggle competitions download -c elo-merchant-category-recommendation\n","download_dataset()"],"execution_count":18,"outputs":[]},{"cell_type":"code","metadata":{"id":"AIWuj3JeoEP7","executionInfo":{"status":"ok","timestamp":1624982365525,"user_tz":-330,"elapsed":394,"user":{"displayName":"Himanshu Shekhar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgMHJJNGV9FNTJGaEvmYm3KHZiIw5OiWV-7atuGjg=s64","userId":"01067290803897199760"}}},"source":["data = pd.read_csv('train.csv', parse_dates=['first_active_month'])"],"execution_count":19,"outputs":[]},{"cell_type":"code","metadata":{"id":"Wl2P_OxSlSHL","executionInfo":{"status":"ok","timestamp":1624982367783,"user_tz":-330,"elapsed":629,"user":{"displayName":"Himanshu Shekhar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgMHJJNGV9FNTJGaEvmYm3KHZiIw5OiWV-7atuGjg=s64","userId":"01067290803897199760"}}},"source":["X = data[0:1]\n","y = data[0:1]['target'].values"],"execution_count":20,"outputs":[]},{"cell_type":"code","metadata":{"id":"RecJVlsbwT6T","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1624983273459,"user_tz":-330,"elapsed":902853,"user":{"displayName":"Himanshu Shekhar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgMHJJNGV9FNTJGaEvmYm3KHZiIw5OiWV-7atuGjg=s64","userId":"01067290803897199760"}},"outputId":"5068e1b7-3a0c-4f96-d7ed-8a61dfa7c191"},"source":["%%time\n","pred_y, rmse = final_function_2(X,y)"],"execution_count":21,"outputs":[{"output_type":"stream","text":["Pipeline started...\n","************************************************************\n","Loading datasets...\n","Mem. usage decreased to 1749.11 Mb (43.7% reduction)\n","Mem. usage decreased to 114.20 Mb (45.5% reduction)\n","All dataset loaded successfully!\n","=======================================================\n","Data preprocessing...\n","All data preprocessed!\n","=======================================================\n","Feature engineering...\n","FE 1 completed!\n","FE 2 completed!\n","FE 3 completed!\n","FE 4 completed!\n","All feature engineering completed!\n","=======================================================\n","Data splitting completed!\n","=======================================================\n","Pipeline completed!\n","************************************************************\n","Loaded best model!\n","CPU times: user 14min 25s, sys: 32.6 s, total: 14min 58s\n","Wall time: 15min 2s\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Kc0Oj_ERfF3p","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1624983279748,"user_tz":-330,"elapsed":423,"user":{"displayName":"Himanshu Shekhar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgMHJJNGV9FNTJGaEvmYm3KHZiIw5OiWV-7atuGjg=s64","userId":"01067290803897199760"}},"outputId":"2d294456-3c39-44bd-e86a-8ff0a23cde71"},"source":["print('Actual target value is:', y)\n","print('Predicted target value is:', pred_y)\n","print('RMSE =', rmse)"],"execution_count":22,"outputs":[{"output_type":"stream","text":["Actual target value is: [-0.8202826]\n","Predicted target value is: [-0.11912893]\n","RMSE = 0.7011536718798138\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"c3xlI0-3eNrM","colab":{"base_uri":"https://localhost:8080/","height":80},"executionInfo":{"status":"ok","timestamp":1624984366729,"user_tz":-330,"elapsed":503,"user":{"displayName":"Himanshu Shekhar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgMHJJNGV9FNTJGaEvmYm3KHZiIw5OiWV-7atuGjg=s64","userId":"01067290803897199760"}},"outputId":"38e28240-46be-4046-fcd6-33c25c3e4d65"},"source":["df = pd.DataFrame({'card_id': X['card_id'].values})\n","df['actual target'] = y\n","df['predicted target'] = pred_y\n","df"],"execution_count":28,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>card_id</th>\n","      <th>actual target</th>\n","      <th>predicted target</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>C_ID_92a2005557</td>\n","      <td>-0.820283</td>\n","      <td>-0.119129</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["           card_id  actual target  predicted target\n","0  C_ID_92a2005557      -0.820283         -0.119129"]},"metadata":{"tags":[]},"execution_count":28}]}]}